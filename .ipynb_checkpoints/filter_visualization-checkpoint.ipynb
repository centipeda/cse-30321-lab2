{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19GUG0PWNU8g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Processing filter 0\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "[_Derived_]{{function_node __forward_restored_function_body_4931}} {{function_node __forward_restored_function_body_4931}} Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node StatefulPartitionedCall/Conv2D}}]]\n\t [[conv2d_11_2/StatefulPartitionedCall]]\n\t [[Mean_2/_553]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-68cdaaf7927c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;31m# we run gradient ascent for 20 steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_img_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0minput_img_data\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrads_value\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3567\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: [_Derived_]{{function_node __forward_restored_function_body_4931}} {{function_node __forward_restored_function_body_4931}} Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node StatefulPartitionedCall/Conv2D}}]]\n\t [[conv2d_11_2/StatefulPartitionedCall]]\n\t [[Mean_2/_553]]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Visualization of the filters of a CNN, via gradient ascent in input space.\n",
    "This script can run on CPU in a few minutes.\n",
    "This script is meant for use only and is not discussed in great details as it is beyond the scope of the class.\n",
    "'''\n",
    "# using tf.2.1 in colab\n",
    "# %tensorflow_version 2.x\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "from tensorflow.keras import backend as K\n",
    "# from google.colab import drive\n",
    "# stopping eager execution mode to be allowed to use the gradient function\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# sanity check for tf version\n",
    "print(tf.__version__)\n",
    "\n",
    "###################################################################################################\n",
    "# dimensions of the generated pictures for each filter.\n",
    "###################################################################################################   \n",
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "###################################################################################################\n",
    "# the name of the layer we want to visualize \n",
    "###################################################################################################   \n",
    "layer_name='conv2d_12'\n",
    "\n",
    "###################################################################################################\n",
    "# util function to convert a tensor into a valid image\n",
    "###################################################################################################   \n",
    "def deprocess_image(x):\n",
    "    \n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "###################################################################################################\n",
    "# Load the model\n",
    "###################################################################################################   \n",
    "from tensorflow.keras.models import load_model\n",
    "model=load_model('./dogs_cats_birds_model_case01_30epoch')\n",
    "\n",
    "###################################################################################################\n",
    "# This is the placeholder for the input images\n",
    "###################################################################################################   \n",
    "input_img = model.input\n",
    "\n",
    "###################################################################################################\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "###################################################################################################   \n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "###################################################################################################\n",
    "# Utility function to normalize a tensor by its L2 norm\n",
    "###################################################################################################   \n",
    "def normalize(x): \n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n",
    "kept_filters = []\n",
    "\n",
    "###################################################################################################\n",
    "# Scan through some number of filters...\n",
    "###################################################################################################   \n",
    "\n",
    "for filter_index in range(64):\n",
    "\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(100):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if True:\n",
    "        \n",
    "    #if loss_value > 0\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "###################################################################################################\n",
    "# we will stich the best n^2 filters on a n x n grid.\n",
    "###################################################################################################\n",
    "n = 5\n",
    "\n",
    "###################################################################################################\n",
    "# the filters that have the highest loss are assumed to be more intuitive\n",
    "# we will only keep the top n filters.\n",
    "###################################################################################################   \n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "###################################################################################################\n",
    "# build a black picture with enough space for\n",
    "# our n x n filters of size 128 x 128, with a 5px margin in between\n",
    "###################################################################################################   \n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "###################################################################################################\n",
    "# fill the picture with our saved filters \n",
    "###################################################################################################   \n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "###################################################################################################\n",
    "# save the result to drive\n",
    "###################################################################################################   \n",
    "save_img('./dogs_cats_birds_model_case01_30epoch_%dx%d.png' % (n, n), stitched_filters)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_17': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x000001713002B908>, 'max_pooling2d_11': <tensorflow.python.keras.saving.saved_model.load.MaxPooling2D object at 0x000001713002D288>, 'conv2d_12': <tensorflow.python.keras.saving.saved_model.load.Conv2D object at 0x0000017130031088>, 'activation_18': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x0000017130031848>, 'max_pooling2d_12': <tensorflow.python.keras.saving.saved_model.load.MaxPooling2D object at 0x00000171300302C8>, 'conv2d_13': <tensorflow.python.keras.saving.saved_model.load.Conv2D object at 0x0000017130030DC8>, 'activation_19': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x00000171300327C8>, 'max_pooling2d_13': <tensorflow.python.keras.saving.saved_model.load.MaxPooling2D object at 0x0000017130033188>, 'conv2d_14': <tensorflow.python.keras.saving.saved_model.load.Conv2D object at 0x0000017130033F88>, 'activation_20': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x00000171300367C8>, 'max_pooling2d_14': <tensorflow.python.keras.saving.saved_model.load.MaxPooling2D object at 0x000001713003C188>, 'conv2d_15': <tensorflow.python.keras.saving.saved_model.load.Conv2D object at 0x000001713003CF88>, 'activation_21': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x000001713003E7C8>, 'max_pooling2d_15': <tensorflow.python.keras.saving.saved_model.load.MaxPooling2D object at 0x000001713003F188>, 'flatten_3': <tensorflow.python.keras.saving.saved_model.load.Flatten object at 0x000001713003FB48>, 'dense_6': <tensorflow.python.keras.saving.saved_model.load.Dense object at 0x000001712FF8B688>, 'activation_22': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x000001712FF8BE08>, 'dense_7': <tensorflow.python.keras.saving.saved_model.load.Dense object at 0x000001712FF60888>, 'activation_23': <tensorflow.python.keras.saving.saved_model.load.Activation object at 0x000001712FF590C8>}\n"
     ]
    }
   ],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U779RFLCUB6k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "filter_visualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
