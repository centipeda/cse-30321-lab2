{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0S1JOLforVjI",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                11214912  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 11,216,003\n",
      "Trainable params: 11,216,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 22290 images belonging to 3 classes.\n",
      "Found 5696 images belonging to 3 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 50 steps, validate for 10 steps\n",
      "Epoch 1/3\n",
      "50/50 [==============================] - 34s 685ms/step - loss: 2.6380 - accuracy: 0.4713 - val_loss: 0.8888 - val_accuracy: 0.5350\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 33s 669ms/step - loss: 0.8237 - accuracy: 0.5858 - val_loss: 0.8797 - val_accuracy: 0.5720\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 32s 642ms/step - loss: 0.7550 - accuracy: 0.6422 - val_loss: 0.7913 - val_accuracy: 0.6090\n",
      "WARNING:tensorflow:From C:\\Users\\criti\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./dogs_cats_birds_model_caseXX_30epoch\\assets\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# using tf.2.1 in colab\n",
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# sanity check for the tf version\n",
    "print(tf.__version__)\n",
    "\n",
    "###################################################################################################\n",
    "# Define image size for network model -- all input images are scaled to this size.\n",
    "###################################################################################################   \n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "###################################################################################################\n",
    "# Include pointers to training and validation data folders -- be sure to examine subfolder structure\n",
    "# --> I also define the # of training samples, validation samples, epochs, and batch size here.\n",
    "###################################################################################################   \n",
    "train_data_dir = './Datasets/data/train'\n",
    "validation_data_dir = './Datasets/data/validation'\n",
    "nb_train_samples = 5000\n",
    "nb_validation_samples = 1000\n",
    "epochs = 3\n",
    "batch_size = 100\n",
    "\n",
    "###################################################################################################\n",
    "# As before, this code simply organizes input data such that channels either come first or last\n",
    "# depending on the backend used (TensorFlow or Theano)\n",
    "###################################################################################################   \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "###################################################################################################\n",
    "# Define our CNN model\n",
    "###################################################################################################\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "################################################\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "###################################################################################################\n",
    "# To process images in respective directories, we can use the ImageDataGenerator class\n",
    "#\tThe code provided here normalizes image data, etc.\n",
    "#\tThe parameters will not be discussed further here, \n",
    "#    but more information / options can be found at:  https://keras.io/preprocessing/image/ \n",
    "###################################################################################################   \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "###################################################################################################\n",
    "# This is the augmentation configuration we will use for testing:  only rescaling\n",
    "###################################################################################################\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "###################################################################################################\n",
    "# Subsequent invocations of flow_from_directory() will use the paths to the training and validation \n",
    "# data, and generate batches of data\n",
    "# \n",
    "# Note that if you wanted to work with grayscale images (for example) you could simply change \n",
    "# color_mode to ‘gray_scale’ (and the number of color channels)\n",
    "# \n",
    "# Again, you will not need to change any parameters here, but a more detailed description of this \n",
    "# class can also be found at the link above.\n",
    "###################################################################################################\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "###################################################################################################\n",
    "# Finally, the invocation of model.fit_generator will simply train the model on batches of data.  \n",
    "# More information can be found at:  https://keras.io/models/sequential/\n",
    "# --> However, this is just analogous to model.fit() discussed in other examples.\n",
    "###################################################################################################\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch= nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "import pandas as pd\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = './dogs_cats_birds_model_case01_30epoch.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "###################################################################################################\n",
    "# Here, we save the model weights and generate an image of the network...\n",
    "###################################################################################################\n",
    "model.save('./dogs_cats_birds_model_caseXX_30epoch')\n",
    "\n",
    "plot_model(model, to_file='./dogs_cats_birds_model_caseXX.png', show_shapes=True, show_layer_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dogs_cats_birds_caseXX.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
